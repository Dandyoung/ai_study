{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/Mistral/Supervised_fine_tuning_(SFT)_of_an_LLM_using_Hugging_Face_tooling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oPAJl2uDmil"
      },
      "source": [
        "## Supervised fine-tuning (SFT) of an LLM\n",
        "\n",
        "집에서 ChatGPT를 만들기 위해서는 세 가지 단계를 거쳐야 합니다:\n",
        "\n",
        "1. 사전 훈련(Pre-training): 인터넷 규모의 데이터에서 다음 토큰을 예측하도록 대형 언어 모델(LLM)을 사전 훈련하는 단계입니다. 이 과정은 수천 개의 GPU 클러스터에서 수행되며, 그 결과물을 \"기본 모델\"이라고 부릅니다.\n",
        "2. 지도학습 미세조정(SFT): 기본 모델을 유용한 어시스턴트로 전환하는 단계입니다.\n",
        "3. 인간 선호도 기반 미세조정: 어시스턴트의 친근함, 유용성 및 안전성을 향상시키는 단계입니다.\n",
        "\n",
        "이 노트북에서는 두 번째 단계인 지도학습 미세조정(SFT), 또는 지침 조정에 대해 설명할 것입니다.\n",
        "\n",
        "지도학습 미세조정은 첫 번째 단계에서 얻은 \"기본 모델\"을 바탕으로, 인터넷 텍스트에서 다음 토큰을 예측하도록 사전 훈련된 모델을 \"챗봇\" 또는 \"어시스턴트\"로 전환하는 과정입니다. 이는 교차 엔트로피 손실을 사용하여 인간의 지침 데이터를 기반으로 모델을 미세조정하는 것을 의미합니다. 다시 말해, 모델은 여전히 다음 토큰을 예측하도록 훈련되지만, 이제는 \"런던에서 할 수 있는 10가지 일은 무엇인가요?\", \"팬케이크를 만드는 방법은?\", \"코끼리에 대한 시를 작성해 주세요\"와 같은 지침에 따라 유용한 답변을 생성하도록 유도됩니다.\n",
        "\n",
        "\n",
        "이를 위해서는 인간 주석자가 유용한 완성 문장을 수집하고, 이를 바탕으로 모델을 훈련시켜야 합니다. 예를 들어, OpenAI는\n",
        "[사람들을 고용하여, 해당 작업을 수행했습니다.](https://gizmodo.com/chatgpt-openai-ai-contractors-15-dollars-per-hour-1850415474) 이들은 \"런던에서는 빅벤을 방문할 수 있습니다(...)\"와 같은 지침에 따라 유용한 완성 문장을 생성하도록 요청받았습니다. 공개적으로 이용 가능한 SFT 데이터셋의 훌륭한 모음은 [여기](https://huggingface.co/collections/HuggingFaceH4/awesome-sft-datasets-65788b571bf8e371c4e4241a)에서 확인할 수 있습니다.\n",
        "\n",
        "Notes:\n",
        "* 전체 노트북은 Hugging Face에서 개발한 [Alignment Handbook](https://github.com/huggingface/alignment-handbook)의 주석이 달린 버전으로 볼 수 있으며\n",
        "특히 Zephyr-7b-beta를 훈련하는 데 사용된 [recipe](https://github.com/huggingface/alignment-handbook/blob/main/recipes/zephyr-7b-beta/sft/config_lora.yaml)을 기반으로 합니다. \n",
        "\n",
        "* 이 노트북은 Transformers 라이브러리에서 사용할 수 있는 모든 디코더 전용 LLM에 적용됩니다. 이 노트북에서는 현재 작성 시점에서 가장 우수한 오픈 소스 대형 언어 모델 중 하나인 [Mistral-7B base model](https://huggingface.co/mistralai/Mistral-7B-v0.1)을 미세조정할 것입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bjdw05Rk5fYm"
      },
      "source": [
        "## Required hardware\n",
        "\n",
        "이 노트북은 [Ampere architecture](https://en.wikipedia.org/wiki/Ampere_(microarchitecture))이상을 지원하며, 최소 24GB의 RAM을 탑재한 NVIDIA GPU에서 실행되도록 설계되었습니다. \n",
        "\n",
        "포함되는 GPU는 다음과 같습니다:\n",
        "\n",
        "* NVIDIA RTX 3090, 4090\n",
        "* NVIDIA A100, H100, H200\n",
        "\n",
        "등등. 개인적으로 저는 24GB RAM을 탑재한 RTX 4090에서 이 노트북을 실행하고 있습니다.\n",
        "\n",
        "암페어 아키텍처를 요구하는 이유는 [bfloat16 (bf16) format](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format)을 사용하기 때문인데, 이는 Turing과 같은 이전 아키텍처에서는 지원되지 않습니다.\n",
        "\n",
        "하지만 약간의 수정으로 float16 (fp16) 포맷을 사용하여 모델을 훈련할 수 있으며, 이는 이전 세대 GPU에서도 지원됩니다. \n",
        "\n",
        "예를 들어:\n",
        "\n",
        "* NVIDIA RTX 2080\n",
        "* NVIDIA Tesla T4\n",
        "* NVIDIA V100.\n",
        "\n",
        "bf16을 fp16으로 변경해야 하는 위치에 대한 주석이 추가되어 있습니다.\n",
        "\n",
        "## Set-up environment\n",
        "\n",
        "지도 학습 미세조정을 수행하기 위해 필요한 모든 🤗 도구들을 설치하는 것부터 시작하겠습니다. 우리는 다음을 사용할 것입니다:\n",
        "\n",
        "* Transformers: 미세조정할 대형 언어 모델(LLM)을 위해\n",
        "* Datasets: 🤗 허브에서 SFT 데이터셋을 로드하고 모델에 맞게 준비하기 위해\n",
        "* BitsandBytes 및 PEFT: 소비자 하드웨어에서 모델을 미세조정하기 위해 [Q-LoRa](https://huggingface.co/blog/4bit-transformers-bitsandbytes)를 활용하는데, 이는 미세조정에 필요한 계산 자원을 크게 줄여주는 기술입니다.\n",
        "* TRL: LLM 미세조정을 위한 유용한 Trainer 클래스를 포함하는 [library](https://huggingface.co/docs/trl/index)입니다.\n",
        "\n",
        "\n",
        "이제 필요한 패키지들을 설치해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-9357hGFRqdi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers[torch] datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rxpq51gW_ySc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q bitsandbytes trl peft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RgDwMbXpC4L"
      },
      "source": [
        "우리는 또한 모델의 어텐션 계산을 가속화하는 [Flash Attention](https://github.com/Dao-AILab/flash-attention)을 설치합니다.\n",
        "\n",
        "\n",
        "* --no-build-isolation 을 쓴 이유는 사람마다 CUDA , Pytorch 버전이 다르기 때문에,, 함부로쓰면 dependency 문제가 생길수도 있습니다~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNJvtR1wGxHm",
        "outputId": "545b75a3-932e-4195-f74a-a6f3447f67de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.6.3.tar.gz (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-^C\n",
            "\u001b[?25canceled\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'no' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install flash-attn --no-build-isolation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[43mno\u001b[49m\u001b[38;5;241m-\u001b[39mbuild\u001b[38;5;241m-\u001b[39misolation\n",
            "\u001b[0;31mNameError\u001b[0m: name 'no' is not defined"
          ]
        }
      ],
      "source": [
        "!pip install flash-attn --no-build-isolation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJIqlyI15kj8"
      },
      "source": [
        "## Load dataset\n",
        "\n",
        "Note :가이드는 여러 데이터셋을 혼합하여 각 데이터셋이 일정 비율의 학습 예제를 포함할 수 있도록 지원합니다. 하지만 Zephyr 레시피는 단일 데이터셋만 사용하며, 해당 데이터셋은 [UltraChat200k dataset](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k)입니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YhYvRDF25j59"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# based on config\n",
        "raw_datasets = load_dataset(\"HuggingFaceH4/ultrachat_200k\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFYvJUfODabt"
      },
      "source": [
        "데이터셋은 여러 분할로 나뉘어 있으며, 각 분할마다 특정 수의 행이 포함되어 있습니다. 저희는 감독된 미세 조정(SFT)을 진행할 예정이기 때문에 \"train_sft\"와 \"test_sft\" 분할만 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jX1sIK6X6Opi",
        "outputId": "f0ab2b40-6789-44c4-ece2-bd84e0d5fa65"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['prompt', 'prompt_id', 'messages'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['prompt', 'prompt_id', 'messages'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "# remove this when done debugging\n",
        "indices = range(0,100)\n",
        "\n",
        "dataset_dict = {\"train\": raw_datasets[\"train_sft\"].select(indices),\n",
        "                \"test\": raw_datasets[\"test_sft\"].select(indices)}\n",
        "\n",
        "raw_datasets = DatasetDict(dataset_dict)\n",
        "raw_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sByKH4hd8mUm"
      },
      "source": [
        "좋습니다. 하나의 예제를 확인해보겠습니다. 각 예제가 메시지 목록을 포함해야 한다는 점이 중요합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2XwVpT17TAb",
        "outputId": "8d441fee-4a0b-4310-9a30-86b4439e0609"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['prompt', 'prompt_id', 'messages'])\n"
          ]
        }
      ],
      "source": [
        "example = raw_datasets[\"train\"][0]\n",
        "print(example.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaWnT4uBCjTy"
      },
      "source": [
        "각 메시지는 두 개의 키를 포함하는 사전(dictionary)입니다:\n",
        "\n",
        "* \"role\": 메시지의 작성자를 지정합니다. 값은 \"system\", \"assistant\", 또는 \"user\"일 수 있습니다. 여기서 \"user\"는 사람을 의미합니다.\n",
        "* \"content\": 메시지의 실제 내용을 담고 있습니다.\n",
        "다음은 이 학습 예제의 메시지 시퀀스를 출력한 예입니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQ1sMda27Zj6",
        "outputId": "04b5ab19-2910-4f1b-91cb-3aece687e49b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user                :  These instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\n",
            "On your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\n",
            "Your Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\n",
            "Does this feature apply to all sections of the theme or just specific ones as listed in the text material?\n",
            "assistant           :  This feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.\n",
            "user                :  Can you guide me through the process of enabling the secondary image hover feature on my Collection pages and Featured Collections sections?\n",
            "assistant           :  Sure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\n",
            "\n",
            "1. Log in to your Shopify account and go to your Online Store.\n",
            "2. Click on Customize theme for the section-based theme you are using.\n",
            "3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\n",
            "4. With the section open, click on the section's setting (gear) icon in the top-left corner.\n",
            "5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\n",
            "6. If available, select 'Show secondary image on hover'.\n",
            "7. Save the changes and preview the Collection/Featured Collection page to see the effect.\n",
            "\n",
            "If you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.\n",
            "user                :  Can you provide me with a link to the documentation for my theme?\n",
            "assistant           :  I don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.\n",
            "user                :  Can you confirm if this feature also works for the Quick Shop section of my theme?\n",
            "assistant           :  The secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\n",
            "\n",
            "1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.\n"
          ]
        }
      ],
      "source": [
        "messages = example[\"messages\"]\n",
        "for message in messages:\n",
        "  role = message[\"role\"]\n",
        "  content = message[\"content\"]\n",
        "  print('{0:20}:  {1}'.format(role, content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DL8S3dkT2Av"
      },
      "source": [
        "이번 경우, 지침이 Shopify에서 특정 기능을 활성화하는 것에 관한 것 같습니다. 흥미롭군요!\n",
        "\n",
        "\n",
        "(Shopify는 종합 상거래 플랫폼이다.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVRxCJQ76spF"
      },
      "source": [
        "## Load tokenizer\n",
        "\n",
        "Next, we instantiate the tokenizer, which is required to prepare the text for the model. The model doesn't directly take strings as input, but rather `input_ids`, which represent integer indices in the vocabulary of a Transformer model. Refer to my [YouTube video](https://www.youtube.com/watch?v=IGu7ivuy1Ag&ab_channel=NielsRogge) if you want to know more about it.\n",
        "\n",
        "\n",
        "다음으로, 모델을 위한 텍스트를 준비하는 데 필요한 토크나이저를 인스턴스화합니다. 모델은 문자열을 직접 입력으로 받지 않고, 대신 Transformer 모델의 어휘 사전에 있는 정수 인덱스를 나타내는 input_ids를 입력으로 받습니다. 이에 대해 더 알고 싶다면 [YouTube video](https://www.youtube.com/watch?v=IGu7ivuy1Ag&ab_channel=NielsRogge)를 참고하세요.\n",
        "\n",
        "\n",
        "또한, 기본 모델의 토크나이저에는 일반적으로 설정되어 있지 않은 몇 가지 속성을 설정합니다. 예를 들어 :\n",
        "\n",
        "- 패딩 토큰 ID: 사전 학습 동안에는 다음 토큰을 예측하기 위해 텍스트 블록을 생성하기 때문에 패딩이 필요 없지만, 미세 조정(fine-tuning) 시에는 (지시문, 완성) 쌍을 동일한 길이의 배치로 만들기 위해 패딩이 필요합니다.\n",
        "- 모델 최대 길이: 모델에 너무 긴 시퀀스를 잘라내기 위해 필요합니다. 여기서는 최대 2048 토큰까지 학습하기로 결정했습니다.\n",
        "- 채팅 템플릿: [chat template](https://huggingface.co/blog/chat-templates)은 각 메시지 목록이 <|user|>와 같은 특수 문자열을 추가하여 토큰화 가능한 문자열로 변환되는 방식을 결정합니다. 이는 사용자 메시지와 챗봇의 응답을 구분하는 역할을 합니다. 여기서는 대부분의 채팅 모델에서 사용하는 기본 채팅 템플릿을 정의합니다. 자세한 내용은 [docs](https://huggingface.co/docs/transformers/main/en/chat_templating)를 참조하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VvIfUqjK6ntu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:778: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)\n",
        "\n",
        "# set pad_token_id equal to the eos_token_id if not set\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Set reasonable default for models without max length\n",
        "if tokenizer.model_max_length > 100_000:\n",
        "    tokenizer.model_max_length = 2048\n",
        "\n",
        "# Set chat template\n",
        "DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '' }}\\n{% endif %}\\n{% endfor %}\"\n",
        "tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UNHQsTJ7O6I"
      },
      "source": [
        "## Apply chat template\n",
        "\n",
        "토크나이저에 적절한 속성을 설정한 후에는 각 메시지 목록에 채팅 템플릿을 적용할 차례입니다. 여기서는 기본적으로 각 (지시문, 완성) 메시지 목록을 모델이 토큰화할 수 있는 문자열로 변환합니다.\n",
        "\n",
        "`tokenize=False`로 설정한 점에 유의하세요. 이는 나중에 정의할 `SFTTrainer`가 내부적으로 토큰화를 수행할 것이기 때문입니다. 여기서는 메시지 목록을 동일한 형식의 문자열로 변환하는 작업만 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44kIpOXa7Ep4",
        "outputId": "2895c3ad-d3c6-481f-8f96-64481dfbb977"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample 87 of the processed training set:\n",
            "\n",
            "\n",
            "</s>\n",
            "\n",
            "Write a news article covering the latest advancements in space exploration. Your article should include recent discoveries, technological developments, and scientific breakthroughs. Use a formal and objective writing style and interview experts in the field for additional insights. The article should also discuss the potential impact of these advancements on future space missions and exploration.</s>\n",
            "\n",
            "In recent years, space exploration has reached new heights with exciting discoveries and technological advancements that have changed our understanding of the universe. From the discovery of exoplanets to the development of more efficient spacecraft, the latest breakthroughs in space exploration are guiding us towards a new era of space science and exploration.\n",
            "\n",
            "One recent discovery that has rocked the space science world is the discovery of the TRAPPIST-1 system, which contains seven Earth-sized planets, three of which are within the habitable zone of the star. This discovery has raised the possibility of there being an extraterrestrial life on planets beyond our solar system. NASA's Spitzer Space Telescope was instrumental in this discovery, as it allowed researchers to study the light emitted by the planets, enabling them to determine their size, distance from their star, and the likelihood of the planet having an atmosphere.\n",
            "\n",
            "Other advances in technology have also made a significant impact on space exploration. The development of ion propulsion systems has led to more efficient spacecraft, which can travel faster and farther than any spacecraft in the past. NASA's Dawn spacecraft was the first to use this technology, exploring two of the largest objects in the asteroid belt, Ceres and Vesta. The successful use of ion propulsion systems has opened the door to more ambitious missions to explore the deep space.\n",
            "\n",
            "Another technological breakthrough that has significantly impacted space exploration is the development of 3D printing technology. One such use of this technology was seen in the International Space Station, where it was used to produce spare parts, tools, and equipment that were not possible to be produced in space earlier. This technology reduces the dependence on shipping parts from Earth and allows for a more sustainable space mission.\n",
            "\n",
            "The latest advancements are not just back on Earth but on the flightpaths too. In April 2021, NASA's Ingenuity helicopter became the first aircraft ever to fly on another planet. This achievement could pave the way for future missions to explore the Martian terrain more efficiently, as aerial vehicles enable scientists to study the planet's surface from a new perspective.\n",
            "\n",
            "The impact of these advancements is substantial and could change the course of space exploration. With the discovery of exoplanets, we may be closer to developing new life-support mechanisms and travel systems that cater to interstellar travel. Ion propulsion technology is a groundwork for future manned space missions to Mars or deep space, where travel time could be reasonably reduced, and using three-day parts when needed will help in surviving missions without depending on supplies from Earth, whilst 3D printing technology will have many benefits for space survival.\n",
            "\n",
            "In conclusion, the advancements in space exploration are advancing rapidly. These breakthroughs are changing our understanding of our discoveries and technological measures, making it easier and efficient to explore beyond the horizon. With such breakthroughs, we may be closer to exploring the far reaches of space and discovering new frontiers like never before.</s>\n",
            "\n",
            "Wow, these advancements in space exploration are really fascinating! I'd love to know more about the ion propulsion systems and how they work. Could you add more information about that?</s>\n",
            "\n",
            "Certainly! Ion propulsion systems are an alternative to traditional chemical rocket engines used in space travel. They work by expelling charged particles, known as ions, at high speeds out of the back of the spacecraft, producing thrust in the opposite direction.\n",
            "\n",
            "Ion engines use electric fields to ionize a propellant, usually xenon or argon gas, which accelerates the ions up to speeds of around 90,000 mph. This is much faster than traditional chemical rockets, which typically only reach speeds of about 25,000 mph.\n",
            "\n",
            "The electric fields are generated by an electric power source, usually solar panels, which supply electricity to the engine's components. The ions are then shot out the back of the engine, propelling the spacecraft forward.\n",
            "\n",
            "The main advantage of ion propulsion systems is that they are much more efficient than traditional chemical rockets, allowing spacecraft to travel farther and faster using less fuel. This opens up new possibilities for exploring deep space and reaching destinations that were previously out of reach.\n",
            "\n",
            "One downside to ion propulsion systems is that they produce relatively low levels of thrust, which means they cannot be used to launch spacecraft into space from Earth's surface. Instead, they are typically used to move spacecraft already in orbit, such as the Dawn spacecraft which traveled to the asteroids Vesta and Ceres.\n",
            "\n",
            "Ion propulsion technology is expected to play a significant role in future human space exploration missions, enabling faster and more efficient travel to destinations such as Mars and beyond.</s>\n",
            "\n",
            "That sounds super interesting. Can you give me some examples of real-life spacecraft that use this technology?</s>\n",
            "\n",
            "Sure! Several spacecraft have used or are currently using ion propulsion technology. Here are a few examples:\n",
            "\n",
            "1. NASA's Dawn spacecraft: This spacecraft used ion thrusters to explore the asteroids Vesta and Ceres from 2007 to 2018. The ion engines allowed Dawn to travel to and orbit these asteroids for extended periods, taking high-resolution images and collecting data on their composition and structure. 2. NASA's Deep Space 1 (DS1) spacecraft: Launched in 1998, DS1 was the first spacecraft to use ion thrusters. The mission tested several advanced technologies, including the ion engines, which allowed the spacecraft to reach speeds of up to 24,000 mph (39,000 km/h). 3. The European Space Agency's (ESA) SMART-1 spacecraft: Launched in 2003, SMART-1 used ion engines to reach the moon, where it orbited and conducted scientific experiments for several months. 4. NASA's BepiColombo spacecraft: Launched in 2018, BepiColombo is on a mission to study the planet Mercury. The spacecraft uses ion engines to adjust its trajectory and reach Mercury, which has a strong gravitational pull. 5. NASA's Juno spacecraft: Launched in 2011, the Juno spacecraft used ion thrusters during its journey to Jupiter, which began in 2016. The ion engines allowed the spacecraft to travel faster and more efficiently than a traditional chemical rocket. These are just a few examples of the many spacecraft that have used or are currently using ion propulsion technology. As advancements in this technology continue, we can expect to see even more ambitious space missions using ion engines in the future.</s>\n",
            "\n",
            "This is all super cool! Can you tell me more about the SMART-1 spacecraft and what it discovered while orbiting the moon?</s>\n",
            "\n",
            "Yes, of course! The SMART-1 mission was launched by the European Space Agency (ESA) in 2003, with the primary goal of demonstrating the use of ion propulsion technology in deep space missions. After a series of spiraling orbits around the Earth, SMART-1 reached the moon in 2004.\n",
            "\n",
            "While in orbit, SMART-1 used a suite of scientific instruments to study the moon's surface and geology. One of its main instruments was a camera that allowed it to capture high-resolution images of the moon's surface. SMART-1 was able to detect the composition of the moon's surface and identify several previously unknown volcanic features.\n",
            "\n",
            "One of the most significant discoveries made by SMART-1 was the detection of deposits of water ice on the moon's surface. The spacecraft's spectrometer detected the presence of hydrogen, which was likely stored in the form of water ice in permanently shadowed craters near the moon's poles. This discovery was important because it suggested that water could be a more common resource in the solar system than previously thought and might be used as a source of fuel or even for supporting human exploration on the moon in the future.\n",
            "\n",
            "SMART-1's mission ended in 2006 when the spacecraft was deliberately crashed into the moon's surface as part of a controlled impact. The impact was carefully monitored by ground-based telescopes and provided valuable data on the moon's geology and structure.\n",
            "\n",
            "Overall, SMART-1's mission was a milestone in the use of ion propulsion technology in space exploration and contributed significantly to our understanding of the moon.</s>\n",
            "\n",
            "Sample 44 of the processed training set:\n",
            "\n",
            "\n",
            "</s>\n",
            "\n",
            "Can you tell me about the traditional music and dance of Southern European countries?</s>\n",
            "\n",
            "Sure, Southern Europe is home to several countries with diverse traditional music and dance styles. Here are some of the most prominent examples:\n",
            "\n",
            "1. Flamenco - Spain's flamenco dance is perhaps the most well-known of Southern European traditional dances. It evolved from the Andalusian region and combines singing, guitar playing, and percussive footwork.\n",
            "\n",
            "2. Tarantella - This fast-paced dance comes from Italy and is often performed in Naples and other parts of Southern Italy. It looks like a frenzied dance, and many believe it can heal the bite of a tarantula spider.\n",
            "\n",
            "3. Syrtos - Greece's Syrtos is a folk dance that dates back to ancient times. It is typically performed with a fast tempo and an upbeat rhythm and is enjoyed at festivals, weddings, and other celebrations.\n",
            "\n",
            "4. Fado - Fado is a melancholy music style from Portugal that originated in Lisbon around the 1820s. It is often performed by a vocalist accompanied by a guitar and has themes of loss, nostalgia, and the sea.\n",
            "\n",
            "5. Sirtaki - Another popular Greek dance, Sirtaki, was made famous worldwide by the movie Zorba the Greek. It is performed in a circle and involves several Greek folk dance steps, accompanied by the song \"Zorba.\"\n",
            "\n",
            "6. Sevillanas - Sevillanas is a traditional dance from the Andalusia region of Spain. This dance is upbeat and joyful, with dancers using their arms to express the emotions of the music.\n",
            "\n",
            "These are just a few of the many traditional music and dance styles of Southern Europe. Each country has its own unique customs and traditions.</s>\n",
            "\n",
            "Can you recommend a good place in Southern Europe to experience traditional music and dance?</s>\n",
            "\n",
            "Yes, there are several great places in Southern Europe that you can visit to experience traditional music and dance. Here are a few options:\n",
            "\n",
            "1. Seville, Spain - Seville is a great place to experience the traditional flamenco dance. The city has numerous flamenco bars where you can see live performances, and there are also several festivals throughout the year that showcase flamenco music and dancing.\n",
            "\n",
            "2. Crete, Greece - Crete is a great destination to experience traditional Greek music and dance. The island has numerous outdoor festivals and celebrations where you can see live performances of Greek folk music and dancing.\n",
            "\n",
            "3. Naples, Italy - Naples is a great place to experience the traditional tarantella dance, which is often performed at festivals and celebrations in the city. The historic center of Naples is a great place to see street performers, musicians, and dancers.\n",
            "\n",
            "4. Lisbon, Portugal - Lisbon is the birthplace of the Fado music style, and there are several live Fado performances throughout the city. Alfama, the old town, is a great place to experience Fado in an intimate setting.\n",
            "\n",
            "5. Dubrovnik, Croatia - Dubrovnik has a rich musical heritage, with many traditional dances still performed today. The city is home to several music festivals during the summer months showcasing traditional Croatian music and dance.\n",
            "\n",
            "These are just a few of the many places you can visit in Southern Europe to experience traditional music and dance.</s>\n",
            "\n",
            "Can you tell me which Southern European country has the most diverse traditional music and dance styles?</s>\n",
            "\n",
            "Southern Europe is home to many countries with diverse traditional music and dance styles. However, if you are looking for a country with a wide range of traditional music and dance styles, then Greece is one of the best options. Greece has a rich musical heritage dating back to ancient times, and there are several different types of traditional Greek music and dance. Some of the most well-known dances include syrtos, kalamatianos, and sirtaki. These dances are typically performed at weddings, festivals, and other celebrations. Greece also has several musical instruments that are unique to the country, including the bouzouki, which is a stringed instrument similar to a mandolin, and the tzouras, which is a smaller version of the bouzouki. In addition to traditional Greek music and dance, there are also several other types of music that have been influenced by Greek culture, including rebetiko, which is a type of music that developed in Greece in the early 20th century and has elements of Turkish, Jewish, and Western music. Overall, Greece offers a rich and diverse range of traditional music and dance styles that are well worth experiencing.</s>\n",
            "\n",
            "Sample 59 of the processed training set:\n",
            "\n",
            "\n",
            "</s>\n",
            "\n",
            "How can the integration of IT systems with procurement in supply chains improve product ordering lead times and pricing provisions? Answer according to: Using the Internet, research information technology (IT) systems for supply chains.\n",
            "-\tProcurement - integration would allow for a clear line of sight into the negotiated terms and conditions, penalties and rewards, lead times for product ordering, as well as the locked pricing and related provisions.\n",
            "This 200 word document outlines the cross functional team that Supply Chain would need to consider integrating IT systems with.</s>\n",
            "\n",
            "The integration of IT systems with procurement in supply chains requires a cross-functional team that involves different stakeholders to ensure reliable and efficient communication, data sharing, and collaboration across the value chain. The team includes IT specialists, procurement managers, logistics experts, procurement analysts, and supply chain strategists. These professionals work together to identify the gaps and inefficiencies in the procurement process and select the right technology to streamline the end-to-end procurement cycle.\n",
            "\n",
            "IT specialists play a critical role in implementing the right software, monitoring the system and providing technical support when needed. The procurement managers provide a clear vision of the procurement objectives and align them with the overall supply chain strategy. The logistics experts collaborate to ensure that the IT systems integrate with the logistics processes, and the procurement analysts provide insights on procurement data and trends that help optimize the procurement processes.\n",
            "\n",
            "Finally, the supply chain strategists ensure the integration of the IT systems with procurement aligns with the organization's overall strategy, objectives and vision. By bringing together a cross-functional team, organizations can successfully integrate IT systems with procurement with a focus on efficiency, visibility, and collaborative relationships with suppliers and logistics service providers.</s>\n",
            "\n",
            "Can you provide examples of IT systems that are commonly used for supply chain management?</s>\n",
            "\n",
            "Sure! There are several IT systems that are commonly used for supply chain management. Here are some examples:\n",
            "\n",
            "1. Enterprise Resource Planning (ERP) - ERP software covers various business operations such as purchasing, accounting, inventory management, production planning, and sales in one integrated system. This helps organizations streamline their business process and track information in real-time.\n",
            "\n",
            "2. Warehouse Management Systems (WMS) - WMS is a software solution that helps manage warehouse operations such as managing inventory, tracking stock levels, optimizing inventory placement, and order fulfillment.\n",
            "\n",
            "3. Transportation Management Systems (TMS) - A TMS solution helps manage transportation operations by optimizing routing, load planning, and carrier selection. It also provides visibility into the shipment status and analytics to identify areas for improvement.\n",
            "\n",
            "4. Supplier Relationship Management (SRM) - An SRM solution provides a platform to manage supplier communication, contracts, performance management, and risk mitigation.\n",
            "\n",
            "5. Electronic Data Interchange (EDI) - EDI solutions facilitate electronic data exchange between organizations, such as purchase orders, invoices, and shipment information, resulting in better communication, improved accuracy and faster processing time.\n",
            "\n",
            "6. Predictive Analytics - Predictive analytics leverages historical data and statistical models to identify insights, forecast demand, optimize inventory, and improve supply chain planning and execution.\n",
            "\n",
            "These are just a few examples of IT systems that can greatly benefit supply chain management. There are several other solutions available in the market catered to specific supply chain functions or industries.</s>\n",
            "\n",
            "Can you explain how an ERP system can help organizations with supply chain management?</s>\n",
            "\n",
            "Sure! Enterprise Resource Planning (ERP) systems can help organizations with supply chain management in several ways. \n",
            "\n",
            "Firstly, an ERP system can help organizations streamline their supply chain operations by integrating various business functions such as purchasing, production, inventory, and logistics into one centralized system. This ensures that all supply chain activities are well-coordinated, reducing the risk of errors, delays, and miscommunications.\n",
            "\n",
            "Secondly, an ERP system can provide real-time visibility into inventory levels, production schedules, and customer orders, allowing organizations to make informed supply chain decisions. This visibility can help organizations optimize their inventory levels, improve demand planning, and ensure timely delivery of goods.\n",
            "\n",
            "Thirdly, an ERP system can help organizations manage vendor relationships and procurement processes more efficiently. By automating purchase orders and supplier payments, for instance, organizations can reduce manual errors and expedite the procurement process.\n",
            "\n",
            "Fourthly, an ERP system can help organizations manage quality control processes and identify potential supply chain risks. This can help organizations mitigate risks more effectively and ensure the quality of goods received from suppliers.\n",
            "\n",
            "Overall, an ERP system provides organizations with a powerful tool for managing their supply chain operations more effectively. With better coordination, real-time visibility, more efficient procurement processes, and risk mitigation capabilities, organizations can reduce costs, improve quality, and enhance customer satisfaction.</s>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import random\n",
        "from multiprocessing import cpu_count\n",
        "\n",
        "def apply_chat_template(example, tokenizer):\n",
        "    messages = example[\"messages\"]\n",
        "    # We add an empty system message if there is none\n",
        "    if messages[0][\"role\"] != \"system\":\n",
        "        messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
        "    example[\"text\"] = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "    return example\n",
        "\n",
        "column_names = list(raw_datasets[\"train\"].features)\n",
        "raw_datasets = raw_datasets.map(apply_chat_template,\n",
        "                                num_proc=cpu_count(),\n",
        "                                fn_kwargs={\"tokenizer\": tokenizer},\n",
        "                                remove_columns=column_names,\n",
        "                                desc=\"Applying chat template\",)\n",
        "\n",
        "# create the splits\n",
        "train_dataset = raw_datasets[\"train\"]\n",
        "eval_dataset = raw_datasets[\"test\"]\n",
        "\n",
        "for index in random.sample(range(len(raw_datasets[\"train\"])), 3):\n",
        "  print(f\"Sample {index} of the processed training set:\\n\\n{raw_datasets['train'][index]['text']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ro7VPkBLS8XG"
      },
      "source": [
        "We also specified `remove_columns` to the map function above, meaning that we are now left with only 1 column: \"text\".\n",
        "\n",
        "Hence the set-up is now very similar to pre-training: we will just train the model predict the next token, given the previous ones. In this case, the model will learn to generate completions given instructions.\n",
        "\n",
        "Hence, similar to pre-training, the labels will be created automatically based on the inputs (by shifting them one position to the right). The model is still trained using cross-entropy. This means that evaluation will mostly be done by checking perplexity/validation loss/model generations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_tvjW-Y-uBT",
        "outputId": "631fdd9f-c4ac-4824-cf6f-7aded04ea5ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7F9-BH4g9sr9"
      },
      "source": [
        "## Define model arguments\n",
        "\n",
        "다음으로, 모델 인자를 정의할 시간입니다.\n",
        "\n",
        "여기서는 모델을 미세 조정(fine-tuning)하는 다양한 방법에 대해 설명이 필요합니다.\n",
        "\n",
        "### Full fine-tuning\n",
        "\n",
        "일반적으로 \"전체 미세 조정\"을 수행합니다. 이는 미세 조정 중에 기본 모델의 모든 가중치를 업데이트한다는 의미입니다. 이는 일반적으로 전체 정밀도(float32) 또는 혼합 정밀도(float32와 float16의 조합)로 수행됩니다. 그러나 LLM과 같은 점점 더 큰 모델의 경우, 이는 실현 불가능해집니다.\n",
        "\n",
        "참고로, float32는 모델의 각 매개변수가 32비트 또는 4바이트로 저장됨을 의미합니다. 예를 들어, Mistral-7B와 같은 70억 매개변수 모델의 경우, 70억 매개변수 × 매개변수당 4바이트 = 280GB의 GPU RAM이 필요합니다. AdamW와 같은 옵티마이저를 사용하여 훈련할 때는 모델뿐만 아니라 그래디언트와 옵티마이저 상태를 위한 메모리도 필요하며, 이는 혼합 정밀도로 훈련할 경우 모델 크기의 약 18배에 해당하는 기가바이트의 메모리가 필요합니다. 이 경우 7B × 18 = 126GB의 GPU RAM이 필요합니다. 이는 단지 70억 매개변수 모델의 경우에 불과합니다! 자세한 내용은 가이드를 참조하세요:https://huggingface.co/docs/transformers/v4.20.1/en/perf_train_gpu_one.\n",
        "\n",
        "### LoRa, a PEFT method\n",
        "\n",
        "따라서, Microsoft의 일부 영리한 연구진은 [LoRa](https://huggingface.co/docs/peft/conceptual_guides/lora) (low-rank adaptation)라는 방법을 고안해냈습니다. 여기서 아이디어는 전체 미세 조정을 수행하는 대신, 기존 모델을 고정하고 모델에 몇 개의 매개변수 가중치(“어댑터”라고 함)를 추가하여 이를 훈련시키는 것입니다.\n",
        "\n",
        "oRa는 매개변수 효율적인 미세 조정(PEFT) 방법이라고 합니다. 이는 몇 개의 어댑터만 훈련시키고 기존 모델은 그대로 두어 매개변수 효율적인 방식으로 모델을 미세 조정하는 인기 있는 방법입니다. LoRa는 Hugging Face의  [PEFT library](https://huggingface.co/docs/peft/v0.7.1/en/index)에 포함되어 있으며, 다양한 다른 PEFT 방법도 지원합니다(하지만 작성 시점에서는 LoRa가 가장 인기 있는 방법입니다).\n",
        "\n",
        "### QLoRa, an even more efficient method\n",
        "\n",
        "일반적인 LoRa의 경우, 기본 모델을 메모리에 32비트 또는 16비트로 유지하고 매개변수 가중치를 훈련시킵니다. 그러나 매개변수당 8비트 또는 4비트로 모델 크기를 크게 줄이는 새로운 방법들이 개발되었습니다(이를  [\"quantization\"](https://huggingface.co/docs/transformers/main_classes/quantization)이라고 합니다). 따라서 LoRa를 양자화된 모델(예: 4비트 모델)에 적용하면 이를 QLoRa라고 부릅니다. 이에 대해 자세히 알고 싶다면 블로그 게시물을 참조하세요. 다양한 양자화 방법이 있지만, 여기서는 [BitsandBytes](https://huggingface.co/docs/transformers/main_classes/quantization#transformers.BitsAndBytesConfig) 통합을 사용할 것입니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XrSQuIyu8Rt1"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# specify how to quantize the model\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n",
        "\n",
        "model_kwargs = dict(\n",
        "    attn_implementation=\"flash_attention_2\", # set this to True if your GPU supports it (Flash Attention drastically speeds up model computations)\n",
        "    torch_dtype=\"auto\",\n",
        "    use_cache=False, # set to False as we're going to use gradient checkpointing\n",
        "    device_map=device_map,\n",
        "    quantization_config=quantization_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5ZvdLgSABbk"
      },
      "source": [
        "## Define SFTTrainer\n",
        "\n",
        "\n",
        "\n",
        "다음으로, TRL 라이브러리에서 제공하는 [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer)를 정의합니다. 이 클래스는 Transformers 라이브러리의 Trainer 클래스를 상속받지만, 감독된 미세 조정(Instruction Tuning)에 최적화되어 있습니다.[Accelerate](https://huggingface.co/docs/accelerate/index)를 백엔드로 사용하여 하나 이상의 GPU에서 즉시 훈련할 수 있습니다.\n",
        "\n",
        "특히, SFTTrainer는 [packing](https://huggingface.co/docs/trl/sft_trainer#packing-dataset--constantlengthdataset-)을 지원합니다. 이는 여러 개의 짧은 예제를 동일한 입력 시퀀스에 패킹하여 훈련 효율성을 높이는 방법입니다.\n",
        "\n",
        "우리가 QLoRa를 사용할 것이므로, PEFT 라이브러리는 어댑터를 적용할 기본 모델의 레이어를 정의하는 편리한 [LoraConfig](https://huggingface.co/docs/peft/v0.7.1/en/package_reference/lora#peft.LoraConfig)를 제공합니다. 일반적으로 Transformer의 어텐션 레이어에 있는 선형 프로젝션 매트릭스에 LoRa를 적용합니다. 그런 다음 이 구성을 SFTTrainer 클래스에 제공합니다. `model_id`를 지정함으로써 기본 모델의 가중치가 로드됩니다(이 과정은 시간이 소요됩니다).\n",
        "\n",
        "\n",
        "또한, 다음과 같은 훈련 관련 다양한 하이퍼파라미터를 설정합니다:\n",
        "\n",
        "* 에폭 동안 미세 조정: 전체 데이터셋을 한 번 학습합니다.\n",
        "* 학습률 및 스케줄러: 학습률과 학습률 스케줄러를 설정하여 모델의 학습 과정을 조절합니다.\n",
        "* 그래디언트 체크포인팅: 훈련 중 메모리를 절약하기 위해 그래디언트 체크포인팅을 사용합니다.\n",
        "* 기타 설정: 배치 크기, 가중치 감소 등 추가적인 하이퍼파라미터를 설정합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "1\n",
            "NVIDIA H100 80GB HBM3\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())  # True가 출력되어야 합니다\n",
        "print(torch.cuda.device_count())  # 사용 가능한 GPU의 수를 출력합니다\n",
        "print(torch.cuda.get_device_name(0))  # GPU 이름을 출력합니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158,
          "referenced_widgets": [
            "ce1f77a753394dc5a25e5470fac18560",
            "7bb6256651a142eabc61984dfe5d379f",
            "12154fd312434260b7f6779a857e1a82",
            "2e44353a59c4480a8e877d842ad16061",
            "abdc9ab22ec049938855373effaf1504",
            "090b3eaf7d2548ee867fc7d9ddf67523",
            "2f2dd26e18ca47dfae4ff33dbb869c0f",
            "e5f6717710074184b78c30f4668be2b5",
            "222a8b16e19140269c44afffbca96865",
            "c3fd940f4dd34ce5b7a462e2bf6f1f71",
            "6264e61a163b4a5dbdb854f7e2ff3056"
          ]
        },
        "id": "W80YklLm_xAY",
        "outputId": "ced661c2-d638-4b4e-bc62-48ca240e2943"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': model_init_kwargs, dataset_text_field, packing, max_seq_length. Will not be supported from version '1.0.0'.\n",
            "\n",
            "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1961: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:152: UserWarning: You passed `model_init_kwargs` to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:174: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
            "  warnings.warn(\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.16s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:181: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1961: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:397: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
            "  warnings.warn(\n",
            "Using auto half precision backend\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Dataset: 100\n",
            "Eval Dataset: 100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 65\n",
            "  Num Epochs = 10,000\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "  Gradient Accumulation steps = 128\n",
            "  Total optimization steps = 10,000\n",
            "  Number of trainable parameters = 54,525,952\n",
            "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [    7/10000 02:08 < 71:07:28, 0.04 it/s, Epoch 4/10000]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.172302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.166906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.164446</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='25' max='62' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25/62 00:03 < 00:06, 6.04 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 62\n",
            "  Batch size = 1\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 62\n",
            "  Batch size = 1\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 62\n",
            "  Batch size = 1\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 62\n",
            "  Batch size = 1\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# 패딩 토큰 설정\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# path where the Trainer will save its checkpoints and logs\n",
        "output_dir = 'data/zephyr-7b-sft-lora'\n",
        "\n",
        "# based on config\n",
        "training_args = TrainingArguments(\n",
        "    fp16=True, # specify bf16=True instead when training on GPUs that support bf16\n",
        "    do_eval=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    gradient_accumulation_steps=128,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    learning_rate=2.0e-05,\n",
        "    log_level=\"info\",\n",
        "    logging_steps=5,\n",
        "    logging_strategy=\"steps\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    max_steps=-1,\n",
        "    num_train_epochs=10000,\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=True,\n",
        "    per_device_eval_batch_size=1, # originally set to 8\n",
        "    per_device_train_batch_size=1, # originally set to 8\n",
        "    # push_to_hub=True,\n",
        "    # hub_model_id=\"zephyr-7b-sft-lora\",\n",
        "    # hub_strategy=\"every_save\",\n",
        "    # report_to=\"tensorboard\",\n",
        "    save_strategy=\"no\",\n",
        "    save_total_limit=None,\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "# based on config\n",
        "peft_config = LoraConfig(\n",
        "        r=64,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "        model=model_id,\n",
        "        model_init_kwargs=model_kwargs,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        dataset_text_field=\"text\",\n",
        "        tokenizer=tokenizer,\n",
        "        packing=True,  # 패킹 활성화\n",
        "        peft_config=peft_config,\n",
        "        max_seq_length=2048,  # max_length 값을 2048로 설정\n",
        "    )\n",
        "\n",
        "# 데이터셋 확인\n",
        "print(\"Train Dataset:\", len(train_dataset))\n",
        "print(\"Eval Dataset:\", len(eval_dataset))\n",
        "\n",
        "# 훈련 시작\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGldALxQIwYu"
      },
      "source": [
        "## Train!\n",
        "\n",
        "마지막으로, 훈련은 `trainer.train()`을 호출하는 것만큼 간단합니다!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgEnI5KMIwyt"
      },
      "outputs": [],
      "source": [
        "train_result = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xxjryHNBKD6"
      },
      "source": [
        "## Saving the model\n",
        "\n",
        "다음으로, Trainer의 상태를 저장합니다. 또한, 로그에 학습 샘플 수를 추가합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ai5jXhJBMsj"
      },
      "outputs": [],
      "source": [
        "metrics = train_result.metrics\n",
        "max_train_samples = training_args.max_train_samples if training_args.max_train_samples is not None else len(train_dataset)\n",
        "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "trainer.save_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tCZxj1tBNAc"
      },
      "source": [
        "## Inference\n",
        "\n",
        "훈련된 모델로 새로운 텍스트를 생성해보겠습니다.\n",
        "\n",
        "추론에는 두 가지 주요 방법이 있습니다:\n",
        "* [pipeline API](https://huggingface.co/docs/transformers/pipeline_tutorial)를 사용하면 전처리 및 후처리에 대한 많은 세부 사항을 추상화하여 간편하게 사용할 수 있습니다. 예를 들어, [model card](https://huggingface.co/HuggingFaceH4/mistral-7b-sft-beta#intended-uses--limitations)가 이를 잘 보여줍니다.\n",
        "* `AutoTokenizer`와 `AutoModelForCausalLM` 클래스를 직접 사용하여 전처리 및 후처리 과정을 직접 구현합니다.\n",
        "\n",
        "우리는 후자의 방법을 선택하여, 내부 동작 방식을 이해해보겠습니다.\n",
        "\n",
        "먼저, 가중치를 저장한 디렉토리에서 모델을 로드합니다. 또한, 4비트 추론을 사용하고, 모델을 사용 가능한 GPU에 자동으로 배치하도록 지정합니다. (`device_map=\"auto\"`에 대한 자세한 내용은 [documentation](https://huggingface.co/docs/accelerate/concept_guides/big_model_inference#the-devicemap)를 참고하세요)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiRvmsSkyubH"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "model = AutoModelForCausalLM.from_pretrained(output_dir, load_in_4bit=True, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7mfwoFnC5zW"
      },
      "source": [
        "다음으로, 토크나이저의 채팅 템플릿을 사용하여 모델에 전달할 메시지 목록을 준비합니다. 여기서 모델이 어떻게 행동해야 하는지를 지정하기 위해 \"system\" 메시지도 추가합니다. 훈련 중에는 모든 대화에 빈 시스템 메시지를 추가했습니다.\n",
        "\n",
        "또한, `add_generation_prompt=True`를 지정하여 모델이 응답을 생성하도록 유도합니다(이는 추론 시에 유용합니다). 입력을 GPU로 이동시키기 위해 \"cuda\"를 지정합니다. 이전에 `device_map=\"auto\"`를 사용했기 때문에 모델은 자동으로 GPU에 배치됩니다.\n",
        "\n",
        "다음으로, [generate()](https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/text_generation#transformers.GenerationMixin.generate)를 사용하여 다음 토큰 ID를 하나씩 자기회귀적으로 생성합니다. 그리디 디코딩(greedy decoding)이나 빔 서치(beam search)와 같은 다양한 생성 전략이 있다는 점에 유의하세요. 자세한 내용은 [이 블로그](https://huggingface.co/blog/how-to-generate)를 참조하세요. 여기서는 샘플링(sampling)을 사용합니다.\n",
        "\n",
        "마지막으로, 토크나이저의 batch_decode 메서드를 사용하여 생성된 토큰 ID를 다시 문자열로 변환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hkacv5PvBOvE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
        "]\n",
        "\n",
        "# prepare the messages for the model\n",
        "input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# inference\n",
        "outputs = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95\n",
        ")\n",
        "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyO3+qbEnufQLsoi2VvofRJ8",
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "090b3eaf7d2548ee867fc7d9ddf67523": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12154fd312434260b7f6779a857e1a82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5f6717710074184b78c30f4668be2b5",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_222a8b16e19140269c44afffbca96865",
            "value": 2
          }
        },
        "222a8b16e19140269c44afffbca96865": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2e44353a59c4480a8e877d842ad16061": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3fd940f4dd34ce5b7a462e2bf6f1f71",
            "placeholder": "​",
            "style": "IPY_MODEL_6264e61a163b4a5dbdb854f7e2ff3056",
            "value": " 2/2 [00:09&lt;00:00,  4.59s/it]"
          }
        },
        "2f2dd26e18ca47dfae4ff33dbb869c0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6264e61a163b4a5dbdb854f7e2ff3056": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7bb6256651a142eabc61984dfe5d379f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_090b3eaf7d2548ee867fc7d9ddf67523",
            "placeholder": "​",
            "style": "IPY_MODEL_2f2dd26e18ca47dfae4ff33dbb869c0f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "abdc9ab22ec049938855373effaf1504": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3fd940f4dd34ce5b7a462e2bf6f1f71": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce1f77a753394dc5a25e5470fac18560": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7bb6256651a142eabc61984dfe5d379f",
              "IPY_MODEL_12154fd312434260b7f6779a857e1a82",
              "IPY_MODEL_2e44353a59c4480a8e877d842ad16061"
            ],
            "layout": "IPY_MODEL_abdc9ab22ec049938855373effaf1504"
          }
        },
        "e5f6717710074184b78c30f4668be2b5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
